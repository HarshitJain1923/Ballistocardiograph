{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae30402-022b-48bb-becb-de725d7e5992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e50b35f5-fd8d-4a62-b492-23f9b22db5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = 'dataset/train'\n",
    "all_subjects = sorted(os.listdir(train_dataset_path))\n",
    "\n",
    "data_frames = []\n",
    "target_data_frames = []\n",
    "\n",
    "for subject_folder in all_subjects:\n",
    "    subject_path = os.path.join(train_dataset_path, subject_folder)\n",
    "    \n",
    "    if os.path.isdir(subject_path):\n",
    "        csv_files = sorted([f for f in os.listdir(subject_path) if f.endswith('.csv')])\n",
    "        \n",
    "        if len(csv_files) >= 2:\n",
    "            first_csv = csv_files[0]\n",
    "            target_csv = csv_files[4]\n",
    "            # Input signal\n",
    "            full_path = os.path.join(subject_path, first_csv)\n",
    "            df = pd.read_csv(full_path, header=None).T\n",
    "            data_frames.append(df)\n",
    "            \n",
    "            # Target values\n",
    "            target_val_path = os.path.join(subject_path, target_csv)\n",
    "            target_df = pd.read_csv(target_val_path, header=None).T\n",
    "            target_df = target_df.drop(index=0).reset_index(drop=True)  # drop first row\n",
    "            target_data_frames.append(target_df)\n",
    "\n",
    "# Combine all subject data\n",
    "train_data_full = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "target_values = pd.concat(target_data_frames, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64c35807-e788-44c3-a206-11afb54e176a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 54240)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54230</th>\n",
       "      <th>54231</th>\n",
       "      <th>54232</th>\n",
       "      <th>54233</th>\n",
       "      <th>54234</th>\n",
       "      <th>54235</th>\n",
       "      <th>54236</th>\n",
       "      <th>54237</th>\n",
       "      <th>54238</th>\n",
       "      <th>54239</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>517</td>\n",
       "      <td>519</td>\n",
       "      <td>520</td>\n",
       "      <td>520</td>\n",
       "      <td>520</td>\n",
       "      <td>519</td>\n",
       "      <td>517</td>\n",
       "      <td>513</td>\n",
       "      <td>509</td>\n",
       "      <td>502</td>\n",
       "      <td>...</td>\n",
       "      <td>484</td>\n",
       "      <td>481</td>\n",
       "      <td>484</td>\n",
       "      <td>470</td>\n",
       "      <td>467</td>\n",
       "      <td>470</td>\n",
       "      <td>466</td>\n",
       "      <td>462</td>\n",
       "      <td>458</td>\n",
       "      <td>464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>504</td>\n",
       "      <td>503</td>\n",
       "      <td>506</td>\n",
       "      <td>506</td>\n",
       "      <td>508</td>\n",
       "      <td>508</td>\n",
       "      <td>511</td>\n",
       "      <td>514</td>\n",
       "      <td>516</td>\n",
       "      <td>517</td>\n",
       "      <td>...</td>\n",
       "      <td>464</td>\n",
       "      <td>469</td>\n",
       "      <td>477</td>\n",
       "      <td>487</td>\n",
       "      <td>493</td>\n",
       "      <td>502</td>\n",
       "      <td>512</td>\n",
       "      <td>522</td>\n",
       "      <td>525</td>\n",
       "      <td>534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>488</td>\n",
       "      <td>489</td>\n",
       "      <td>487</td>\n",
       "      <td>487</td>\n",
       "      <td>486</td>\n",
       "      <td>484</td>\n",
       "      <td>484</td>\n",
       "      <td>484</td>\n",
       "      <td>484</td>\n",
       "      <td>486</td>\n",
       "      <td>...</td>\n",
       "      <td>271</td>\n",
       "      <td>256</td>\n",
       "      <td>245</td>\n",
       "      <td>240</td>\n",
       "      <td>239</td>\n",
       "      <td>239</td>\n",
       "      <td>239</td>\n",
       "      <td>244</td>\n",
       "      <td>248</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>646</td>\n",
       "      <td>652</td>\n",
       "      <td>652</td>\n",
       "      <td>653</td>\n",
       "      <td>650</td>\n",
       "      <td>645</td>\n",
       "      <td>637</td>\n",
       "      <td>626</td>\n",
       "      <td>616</td>\n",
       "      <td>604</td>\n",
       "      <td>...</td>\n",
       "      <td>525</td>\n",
       "      <td>524</td>\n",
       "      <td>524</td>\n",
       "      <td>524</td>\n",
       "      <td>525</td>\n",
       "      <td>522</td>\n",
       "      <td>520</td>\n",
       "      <td>521</td>\n",
       "      <td>521</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>467</td>\n",
       "      <td>469</td>\n",
       "      <td>467</td>\n",
       "      <td>472</td>\n",
       "      <td>469</td>\n",
       "      <td>471</td>\n",
       "      <td>476</td>\n",
       "      <td>479</td>\n",
       "      <td>482</td>\n",
       "      <td>484</td>\n",
       "      <td>...</td>\n",
       "      <td>482</td>\n",
       "      <td>479</td>\n",
       "      <td>475</td>\n",
       "      <td>471</td>\n",
       "      <td>468</td>\n",
       "      <td>464</td>\n",
       "      <td>464</td>\n",
       "      <td>460</td>\n",
       "      <td>459</td>\n",
       "      <td>458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54240 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2      3      4      5      6      7      8      9      ...  \\\n",
       "0    517    519    520    520    520    519    517    513    509    502  ...   \n",
       "1    504    503    506    506    508    508    511    514    516    517  ...   \n",
       "2    488    489    487    487    486    484    484    484    484    486  ...   \n",
       "3    646    652    652    653    650    645    637    626    616    604  ...   \n",
       "4    467    469    467    472    469    471    476    479    482    484  ...   \n",
       "\n",
       "   54230  54231  54232  54233  54234  54235  54236  54237  54238  54239  \n",
       "0    484    481    484    470    467    470    466    462    458    464  \n",
       "1    464    469    477    487    493    502    512    522    525    534  \n",
       "2    271    256    245    240    239    239    239    244    248    259  \n",
       "3    525    524    524    524    525    522    520    521    521    521  \n",
       "4    482    479    475    471    468    464    464    460    459    458  \n",
       "\n",
       "[5 rows x 54240 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_data_full.shape)\n",
    "train_data_full.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07a556a-ae88-41db-8709-86b7f77361d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 240)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 240 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  230  231  232  233  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "1    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "2    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "3    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "4    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "\n",
       "   234  235  236  237  238  239  \n",
       "0    0    0    0    0    0    0  \n",
       "1    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    0  \n",
       "3    0    0    0    0    0    0  \n",
       "4    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 240 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(target_values.shape)\n",
    "target_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "034d2e8a-cf20-4785-8223-3c4409f2c9cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(target_values.values.flatten(), bins=20)\n",
    "plt.title(\"Distribution\")\n",
    "plt.xlabel(\"Values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2a2b46f-3387-48a9-8d5a-82282216e31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of ones in dataset: 351\n"
     ]
    }
   ],
   "source": [
    "total_ones = target_values.values.sum()\n",
    "print(\"Total number of ones in dataset:\", total_ones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37c36d2c-b3a1-4b88-9570-2cb974821860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 out of 46 samples have at least one positive label.\n"
     ]
    }
   ],
   "source": [
    "non_zero_rows = (target_values.sum(axis=1) > 0).sum()\n",
    "print(f\"{non_zero_rows} out of {target_values.shape[0]} samples have at least one positive label.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36beb7e0-f6bd-4a51-a416-0831915274f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, n_hidden=512):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=226, hidden_size=n_hidden, batch_first=True)\n",
    "        self.linear = nn.Linear(n_hidden, 1)  # One output per timestep\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, 240, 226]\n",
    "        lstm_out, _ = self.lstm(x)  # [batch, 240, n_hidden]\n",
    "        logits = self.linear(lstm_out).squeeze(-1)  # [batch, 240]\n",
    "        return logits  # raw logits (use BCEWithLogitsLoss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6a30c00-7ae8-4f7c-b8f7-ad0715c43eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovementDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data  # shape: [n_subjects, 240*226]\n",
    "        self.targets = targets  # shape: [n_subjects, 240]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # reshape signal into [240, 226]\n",
    "        signal = self.data.iloc[idx].values.astype(np.float32).reshape(240, 226)\n",
    "        signal = torch.tensor(signal)  # [240, 226]\n",
    "\n",
    "        target = self.targets.iloc[idx].values.astype(np.float32)  # [240]\n",
    "        target = torch.tensor(target)  # [240]\n",
    "\n",
    "        return signal, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fa98134-15cd-4e7d-8c1e-3850763e2fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split both data and target labels\n",
    "traindata, valdata, train_targets, val_targets = train_test_split(\n",
    "    train_data_full, target_values, test_size=0.16, random_state=42\n",
    ")\n",
    "\n",
    "# Create datasets using corresponding targets\n",
    "train_dataset = MovementDataset(traindata, train_targets)\n",
    "val_dataset = MovementDataset(valdata, val_targets)\n",
    "\n",
    "# DataLoaders\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5120eb41-475b-4ee8-a5fe-f7eeb29789ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCEWithLogitsLoss\n",
    "import torch\n",
    "\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.bce = BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = self.bce(inputs, targets)\n",
    "        pt = torch.exp(-bce_loss)  # pt = 1 - p_t\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        return focal_loss.mean() if self.reduction == 'mean' else focal_loss.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d6540b1-eb91-4fba-97eb-34bd8155e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def train(model, lossfn, optimizer, scheduler, device, train_dataloader, val_dataloader, epochs=50, threshold=0.5):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        all_preds = []\n",
    "        all_trues = []\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for t, (xb, yb) in enumerate(train_dataloader):\n",
    "            xb = xb.to(device, dtype=torch.float32)  # [batch, 240, 226]\n",
    "            yb = yb.to(device, dtype=torch.float32)  # [batch, 240]\n",
    "\n",
    "            logits = model(xb)  # Output: [batch, 240]\n",
    "            loss = lossfn(logits, yb)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            preds_binary = (torch.sigmoid(logits) > threshold).int().cpu().numpy()\n",
    "            yb_cpu = yb.int().cpu().numpy()\n",
    "\n",
    "            all_preds.append(preds_binary)\n",
    "            all_trues.append(yb_cpu)\n",
    "\n",
    "        # Compute F1 score (train)\n",
    "        all_preds = np.concatenate(all_preds, axis=0).flatten()\n",
    "        all_trues = np.concatenate(all_trues, axis=0).flatten()\n",
    "        f1_micro = f1_score(all_trues, all_preds, average='micro', zero_division=0)\n",
    "\n",
    "        print(f\"Train Loss: {epoch_loss:.4f} | Train F1 : {f1_micro:.4f}  | Unique preds: {np.unique(all_preds)}\")\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_trues = []\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xv, yv in val_dataloader:\n",
    "                xv = xv.to(device, dtype=torch.float32)\n",
    "                yv = yv.to(device, dtype=torch.float32)\n",
    "\n",
    "                logits = model(xv)\n",
    "                loss = lossfn(logits, yv)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds_binary = (torch.sigmoid(logits) > threshold).int().cpu().numpy()\n",
    "                yv_cpu = yv.int().cpu().numpy()\n",
    "\n",
    "                all_preds.append(preds_binary)\n",
    "                all_trues.append(yv_cpu)\n",
    "\n",
    "        all_preds = np.concatenate(all_preds, axis=0).flatten()\n",
    "        all_trues = np.concatenate(all_trues, axis=0).flatten()\n",
    "        f1 = f1_score(all_trues, all_preds, average='micro', zero_division=0)\n",
    "\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val F1 : {f1:.4f} | Unique preds: {np.unique(all_preds)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c223d832-d173-4afe-a192-03937af38505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = LSTM()  # Assumes input [batch, 240, 226] and output [batch, 240]\n",
    "\n",
    "# Calculate class balance\n",
    "pos = np.sum(train_targets.values)\n",
    "neg = np.prod(train_targets.shape) - pos\n",
    "pos_weight = torch.tensor([neg / pos]).to(device)\n",
    "\n",
    "lossfn = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "\n",
    "# Optimizer with a small learning rate (recommended for LSTM)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Scheduler to reduce LR if val loss plateaus\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',          # watch validation loss\n",
    "    patience=7,          # wait 7 epochs before reducing\n",
    "    factor=0.5,          # halve the LR\n",
    "    eps=1e-11            # minimal LR change\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98abd396-ff9c-442b-a75b-a95b91a15db7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n",
      "Train Loss: 2.1234 | Train F1 : 0.9473  | Unique preds: [0 1]\n",
      "Val Loss: 0.6249 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 2/50\n",
      "Train Loss: 1.8802 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.6814 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 3/50\n",
      "Train Loss: 1.9277 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.6656 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 4/50\n",
      "Train Loss: 1.8416 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.6407 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 5/50\n",
      "Train Loss: 1.7566 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.5946 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 6/50\n",
      "Train Loss: 1.7191 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.5786 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 7/50\n",
      "Train Loss: 1.6196 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.5395 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 8/50\n",
      "Train Loss: 1.5940 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.5273 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 9/50\n",
      "Train Loss: 1.5569 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.5129 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 10/50\n",
      "Train Loss: 1.5404 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4999 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 11/50\n",
      "Train Loss: 1.5155 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4933 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 12/50\n",
      "Train Loss: 1.5069 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4892 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 13/50\n",
      "Train Loss: 1.5173 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4927 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 14/50\n",
      "Train Loss: 1.5063 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4873 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 15/50\n",
      "Train Loss: 1.5128 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4947 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 16/50\n",
      "Train Loss: 1.5135 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4933 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 17/50\n",
      "Train Loss: 1.5044 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4938 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 18/50\n",
      "Train Loss: 1.5079 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4888 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 19/50\n",
      "Train Loss: 1.5026 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4877 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 20/50\n",
      "Train Loss: 1.5038 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4884 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 21/50\n",
      "Train Loss: 1.4984 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4934 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 22/50\n",
      "Train Loss: 1.5031 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4924 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 23/50\n",
      "Train Loss: 1.4985 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4920 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 24/50\n",
      "Train Loss: 1.4996 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4893 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 25/50\n",
      "Train Loss: 1.4968 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4878 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 26/50\n",
      "Train Loss: 1.4991 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4898 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 27/50\n",
      "Train Loss: 1.4989 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4882 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 28/50\n",
      "Train Loss: 1.4915 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4863 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 29/50\n",
      "Train Loss: 1.4977 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4904 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 30/50\n",
      "Train Loss: 1.4934 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4890 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 31/50\n",
      "Train Loss: 1.4919 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4880 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 32/50\n",
      "Train Loss: 1.4901 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4868 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 33/50\n",
      "Train Loss: 1.4923 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4857 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 34/50\n",
      "Train Loss: 1.4882 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4844 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 35/50\n",
      "Train Loss: 1.4867 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4840 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 36/50\n",
      "Train Loss: 1.4861 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4837 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 37/50\n",
      "Train Loss: 1.4857 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4834 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 38/50\n",
      "Train Loss: 1.4851 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4830 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 39/50\n",
      "Train Loss: 1.4848 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4828 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 40/50\n",
      "Train Loss: 1.4842 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4827 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 41/50\n",
      "Train Loss: 1.4837 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4824 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 42/50\n",
      "Train Loss: 1.4834 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4820 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 43/50\n",
      "Train Loss: 1.4825 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4819 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 44/50\n",
      "Train Loss: 1.4824 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4819 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 45/50\n",
      "Train Loss: 1.4823 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4819 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 46/50\n",
      "Train Loss: 1.4821 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4818 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 47/50\n",
      "Train Loss: 1.4820 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4815 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 48/50\n",
      "Train Loss: 1.4817 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4814 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 49/50\n",
      "Train Loss: 1.4816 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4812 | Val F1 : 0.9542 | Unique preds: [0]\n",
      "\n",
      "Epoch 50/50\n",
      "Train Loss: 1.4814 | Train F1 : 0.9769  | Unique preds: [0]\n",
      "Val Loss: 0.4811 | Val F1 : 0.9542 | Unique preds: [0]\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=model,\n",
    "    lossfn=lossfn,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    epochs=50,           # or more depending on your need\n",
    "    threshold=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e05a80-1b60-4a4b-9c4d-9ceaa22d5824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
